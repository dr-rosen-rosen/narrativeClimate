{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling, BertForMaskedLM, TrainingArguments, Trainer\n",
    "import torch, torchvision\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Set up GPU backend\n",
    "# https://pytorch.org/docs/main/notes/mps.html\n",
    "# https://stackoverflow.com/questions/63423463/using-pytorch-cuda-on-macbook-pro\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "    # output expected:\n",
    "    # tensor([1.], device='mps:0')\n",
    "\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516259\n",
      "448453\n",
      "387683\n"
     ]
    }
   ],
   "source": [
    "# Read in events, clean and store as dataset\n",
    "# def clean_text(event):\n",
    "#     event = re.sub(r'-+',' ',event)\n",
    "#     event = re.sub(r'[^a-zA-Z, ]+',\" \",event)\n",
    "#     event = re.sub(r'[ ]+',\" \",event)\n",
    "#     line += \".\"\n",
    "#     return line\n",
    "df = pd.read_csv('eid_eventText.csv', usecols=['eid','event_text']).dropna(how='any',axis=0)\n",
    "print(len(df))\n",
    "df.drop_duplicates(subset=['event_text'],keep='first',inplace=True)\n",
    "print(len(df))\n",
    "df = df[df['event_text'].str.split().str.len().gt(20)] # drops events with fewer than 20 words  \n",
    "df.rename(columns={\"event_text\":\"text\"},inplace=True)\n",
    "print(len(df))\n",
    "dataset = Dataset.from_pandas(df).shuffle(seed=242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 387683/387683 [01:14<00:00, 5201.55 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(reports):\n",
    "    return tokenizer(reports[\"text\"], return_tensors=\"np\", truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# Setup model\n",
    "# https://github.com/google-research/bert#pre-training-with-bert\n",
    "# https://huggingface.co/learn/nlp-course/chapter3/3\n",
    "# https://huggingface.co/blog/pretraining-bert\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#overview\n",
    "# https://www.kaggle.com/code/thierryneusius/pretraining-bert-with-hugging-face-transformers\n",
    "# https://www.analyticsvidhya.com/blog/2022/09/fine-tuning-bert-with-masked-language-modeling/\n",
    "# https://huggingface.co/docs/transformers/en/training\n",
    "# https://huggingface.co/docs/transformers/en/tasks/masked_language_modeling\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "# model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('./event_trainer/checkpoint-43500') \n",
    "\n",
    "# Evaluation \n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Data\n",
    "train_eval_ds = tokenized_ds.train_test_split(test_size=0.1, shuffle=True,seed=42)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "# Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"event_trainer\", \n",
    "    eval_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_eval_ds[\"train\"],\n",
    "    eval_dataset= train_eval_ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130845 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 500/130845 [03:18<14:19:14,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.368, 'grad_norm': 7.638306140899658, 'learning_rate': 4.9808934235163746e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1000/130845 [06:38<14:22:25,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3508, 'grad_norm': 7.813417911529541, 'learning_rate': 4.961786847032749e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1500/130845 [09:58<14:18:21,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3534, 'grad_norm': 6.1073503494262695, 'learning_rate': 4.942680270549123e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 2000/130845 [13:17<14:08:31,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3235, 'grad_norm': 4.402304649353027, 'learning_rate': 4.9235736940654976e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 2500/130845 [16:35<13:56:24,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3254, 'grad_norm': 7.6424713134765625, 'learning_rate': 4.904467117581872e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 2923/130845 [19:24<14:03:03,  2.53it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
